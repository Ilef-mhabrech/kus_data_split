{
  "title": "AR-SEP: Sustainability Issues",
  "Illustrative Learning Outcomes": {
    "Non-core": "1. Assess the environmental impacts of a given project’s deployment (e.g., the energy consumption of\nCPUs and GPUs, contribution to e-waste, and effect of hardware virtualization in data centers).\nProfessional Dispositions\n● Self-directed: Students should increasingly become self-motivated to acquire complementary\nknowledge.\n● Proactive: Students should exercise control and anticipate issues related to the underlying\ncomputer system.\n109\nMathematics Requirements\n● MSF-Discrete, MSF-Linear, MSF-Statistics, MSF-Calculus, MSF-Probability\nCourse Packaging Suggestions\nComputer Architecture - Introductory Course to include the following:\n● SEP-History (2 hours)\n● AR-Representation (2 hours)\n● AR-Assembly (2 hours)\n● AR-Memory (10 hours)\n● OS-Memory (10 hours)\n● AR-IO (4 hours)\n● AR-Heterogeneity (5 hours)\n● PDC-Programs (4 hours)\n● SEP-Ethical-Analysis (3 hours)\nCourse objectives: Students should understand the fundamentals of modern computer architectures,\nincluding the challenges associated with memory caches, memory management, and pipelining.\nPrerequisites:\n● MSF-Discrete\nComputer Architecture - Advanced Topics Course to include the following:\n● AR-Logic (4 hours)\n● AR-Representation (2 hours)\n● AR-Assembly (2 hours)\n● AR-Memory (10 hours)\n● AR-IO (2 hours)\n● SF-Performance (4 hours)\n● AR-Heterogeneity (4 hours)\n● AR-Performance-Energy (5 hours)\n● AR-Security (4 hours)\n● AR-Quantum (4 hours)\nCourse objectives: Students should understand how computer architectures evolved into today’s\nheterogeneous systems and to what extent choices made in the past can influence the design of future\nhigh-performance computing systems.\nPrerequisites:\n● MSF-Discrete\nSystems Course to include the following:\n● SEP-History (2 hours)\n110\n● SF-Design (2 hours)\n● SF-Reliability (2 hours)\n● OS-Purpose (2 hours)\n● AR-Representation (2 hours)\n● AR-Assembly (2 hours)\n● AR-Memory (8 hours)\n● AR-IO (2 hours)\n● PDC-Algorithms (4 hours)\n● AR-Heterogeneity (4 hours)\n● AR-Performance-Energy (5 hours)\n● NC-Applications (5 hours)\nCourse objectives: Students should understand the advanced architectural aspects of modern\ncomputer systems, including heterogeneous architectures and the required hardware and software\ninterfaces to improve the performance and energy footprint of applications.\nPrerequisites:\n● MSF-Discrete, MSF-Statistics\nCommittee\nChair: Marcelo Pias, Federal University of Rio Grande (FURG), Rio Grande-RS, Brazil\nMembers:\n● Brett A. Becker, University College Dublin, Dublin, Ireland\n● Mohamed Zahran, New York University, New York, NY, USA\n● Monica D. Anderson, University of Alabama, Tuscaloosa, AL, USA\n● Qiao Xiang, Xiamen University, Xiamen, China\n● Adrian German, Indiana University, Bloomington, IN, USA\n111\n112\nData Management (DM)\nPreamble\nSince the mid-1970s, the study of Data Management (DM) has meant an almost exclusive study of\nrelational database systems. Depending on institutional context, students have studied, in varying\nproportions, the following.\n• Data modeling and database design: for example, E-R Data model, relational model,\nnormalization theory\n• Query construction: e.g., relational algebra, SQL\n• Query processing: e.g., indices (B+tree, hash), algorithms (e.g., external sorting, select, project,\njoin), query optimization (transformations, index selection)\n• DBMS internals: e.g., concurrency/locking, transaction management, buffer management\nToday's graduates are expected to possess DBMS user (rather than implementor) skills. These\nprimarily include data modeling and query construction; ability to take an unorganized collection of data,\norganize it using a DBMS, and access/update the collection via queries.\nAdditionally, students need to study the following.\n● The role data plays in an organization. This includes the Data Life Cycle: Creation-Processing-\nReview/Reporting-Retention/Retrieval-Destruction.\n● The social/legal aspects of data collection: e.g., scale, data privacy, database privacy (compliance)\nby design, de-identification, ownership, reliability, database security, and intended and unintended\napplications.\n● Emerging and advanced technologies that are augmenting/replacing traditional relational systems,\nparticularly those used to support (big) data analytics, including NoSQL (e.g., JSON, XML, key-\nvalue store databases), cloud databases, MapReduce, and dataframes.\n● The existing and emerging roles for those involved with data management, which include the\nfollowing.\no Product feature engineers: those who use both SQL and NoSQL operational databases.\no Analytical engineers/data engineers: those who write analytical SQL, Python, and Scala\ncode to build data assets for business groups.\no Business analysts: those who build/manage data most frequently with Excel spreadsheets.\no Data infrastructure engineers: those who implement a data management system in a variety\nof data applications (e.g., OLTP).\no “Everyone” who produces or consumes data must understand the associated social, ethical,\nand professional issues.\nOne role that transcends all the above categories is that of data custodian. Previously, data were seen\nas a resource to be managed (Information Systems Management) just like other enterprise resources.\nToday, data are seen in a larger context. Data about customers can now be seen as belonging to (or in\nsome national contexts, as owned by) those customers. There is now an accepted understanding that\nthe safe and ethical storage, and use, of institutional data is part of being a responsible data custodian.\n113\nFurthermore, we acknowledge the tension between a curricular focus on professional preparation\nversus the study of a knowledge area as a scientific endeavor. This is particularly true with Data\nManagement. For example, proving (or at least knowing) the completeness of Armstrong’s Axioms is\nfundamental in functional dependency theory. However, most computer science graduates will never\nutilize this concept during their professional careers. The same can be said for many other topics in the\nData Management canon. Conversely, if our graduates can only normalize data into Boyce-Codd\nnormal form (using an automated tool) and write SQL queries, without understanding the role that\nindices play in efficient query execution, we have done them and society a disservice.\nTo this end, the number of CS Core hours is relatively small relative to the KA Core hours. This\napproach is designed to allow institutions with differing contexts to customize their curricula\nappropriately. An institution that focuses on OLTP implementation, for example, would prioritize efficient\nstorage and data access, while an institution that focuses on product features would prioritize\nprogrammatic access to extant databases.\nHowever, an institution manages this tension, we wish to give voice to one of the ironies of computer\nscience curricula. Students typically spend much of their educational life reading (and writing) data from\na file or interactively, while outside of the academy the predominant data comes from databases\naccessed programmatically. Perhaps in the not-too-distant future students will learn programmatic\ndatabase access early on and then continue this practice as they progress through their curriculum.\nFinally, we understand that while the Data Management KA may be orthogonal to the SEC (Security)\nand SEP (Society, Ethics, and the Profession) KAs, it is also ground zero for these (and other)\nknowledge areas. When designing persistent data stores, the question of what should be stored must\nbe examined from both legal and ethical perspectives. Are there privacy concerns? And just as\nimportantly, how well protected is the data?\nChanges since CS2013\n● Rename the knowledge area from Information Management to Data Management. This\nrenaming does not represent any kind of philosophical shift. It is simply an effort to avoid\nconfusion with the similar definitions used in Information Systems and Information Technology\ncurricula.\n● Inclusion of NoSQL approaches and MapReduce as CS Core topics.\n● Increased attention to SEP and SEC topics in both the CS Core and KA Core areas.\nCore Hours\nKnowledge Unit CS Core Hours KA Core Hours\nThe Role of Data 2\nCore Database Systems Concepts 2 1\n114\nData Modeling 2 3\nRelational Databases 1 3\nQuery Construction 2 4\nQuery Processing 4\nDBMS Internals 4\nNoSQL Systems 2\nData Security & Privacy 1 2\nData Analytics 3\nDistributed Databases/Cloud Computing\nSemi-structured and Unstructured Databases\nSociety, Ethics, and the Profession Included in SEP hours\nTotal 10 26\nThe CS Core hour in Data Security & Privacy is shared with SEC and is counted here.\nKnowledge Units"
  }
}