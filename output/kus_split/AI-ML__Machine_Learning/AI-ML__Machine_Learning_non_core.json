{
  "title": "AI-ML: Machine Learning",
  "Non-core": "17. General statistical-based learning, parameter estimation (maximum likelihood)\n18. Supervised learning\na. Decision trees\nb. Nearest-neighbor classification and regression\nc. Learning simple neural networks / multi-layer perceptrons\nd. Linear regression\ne. Logistic regression\nf. Support vector machines (SVMs) and kernels\ng. Gaussian Processes\n19. Overfitting\na. The curse of dimensionality\nb. Regularization (mathematical computations, L and L regularization)\n2 1\n20. Experimental design\n72\na. Data preparation (e.g., standardization, representation, one-hot encoding)\nb. Hypothesis space\nc. Biases (e.g., algorithmic, search)\nd. Partitioning data: stratification, training set, validation set, test set\ne. Parameter tuning (grid/random search, via cross-validation)\nf. Performance evaluation\ni. Cross-validation\nii. Metric: error, precision, recall, confusion matrix\niii. Receiver operating characteristic (ROC) curve and area under ROC curve\n21. Bayesian learning (Cross-Reference AI/Reasoning Under Uncertainty)\na. Naive Bayes and its relationship to linear models\nb. Bayesian networks\nc. Prior/posterior\nd. Generative models\n22. Deep learning\na. Deep feed-forward networks\nb. Neural tangent kernel and understanding neural network training\nc. Convolutional neural networks\nd. Autoencoders\ne. Recurrent networks\nf. Representations and knowledge transfer\ng. Adversarial training and generative adversarial networks\nh. Attention mechanisms\n23. Representations\na. Manually crafted representations\nb. Basis expansion\nc. Learned representations (e.g., deep neural networks)\n24. Unsupervised learning and clustering\na. K-means\nb. Gaussian mixture models\nc. Expectation maximization (EM)\nd. Self-organizing maps\n25. Graph analysis (e.g., PageRank)\n26. Semi-supervised learning\n27. Graphical models (See also: AI-Probability)\n28. Ensembles\na. Weighted majority\nb. Boosting/bagging\nc. Random forest\nd. Gated ensemble\n29. Learning theory\na. General overview of learning theory / why learning works\nb. VC dimension\nc. Generalization bounds\n73\n30. Reinforcement learning\na. Exploration vs exploitation tradeoff\nb. Markov decision processes\nc. Value and policy iteration\nd. Policy gradient methods\ne. Deep reinforcement learning\nf. Learning from demonstration and inverse RL\n31. Explainable / interpretable machine learning\na. Understanding feature importance (e.g., LIME, Shapley values)\nb. Interpretable models and representations\n32. Recommender systems\n33. Hardware for machine learning\na. GPUs / TPUs\n34. Application of machine learning algorithms to:\na. Medicine and health\nb. Economics\nc. Education\nd. Vision\ne. Natural language\nf. Robotics\ng. Game play\nh. Data mining (Cross-reference DM/Data Analytics)\n35. Ethics for Machine Learning\na. Continued focus on real data, real scenarios, and case studies (See also: SEP-Context)\nb. In depth exploration of dataset/algorithmic/evaluation bias, data privacy, and fairness (See also:\nSEP-Privacy, SEP-Context)\nc. Trust / explainability"
}