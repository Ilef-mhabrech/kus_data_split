{
  "title": "MSF-Linear: Linear Algebra",
  "Illustrative Learning Outcomes": {
    "CS Core": "1. Vectors: definitions, vector operations, geometric interpretation, angles\na. Describe algebraic and geometric representations of vectors in Rn and their operations,\nincluding addition, scalar multiplication, and dot product.\nb. List properties of vectors in Rn.\nc. Compute angles between vectors in Rn.",
    "KA Core": "2. Matrices, matrix-vector equation, geometric interpretation, geometric transformations with matrices\na. Perform common matrix operations, such as addition, scalar multiplication, multiplication, and\ntransposition.\nb. Relate a matrix to a homogeneous system of linear equations.\nc. Recognize when two matrices can be multiplied.\nd. Relate various matrix transformations to geometric illustrations.\n3. Solving equations, row-reduction\na. Formulate, solve, apply, and interpret properties of linear systems.\nb. Perform row operations on a matrix.\nc. Relate an augmented matrix to a system of linear equations.\nd. Solve linear systems of equations using the language of matrices.\ne. Translate word problems into linear equations.\nf. Perform Gaussian elimination.\n4. Linear independence, span, basis\na. Define subspace of a vector space.\n191\nb. List examples of subspaces of a vector space.\nc. Recognize and use basic properties of subspaces and vector spaces.\nd. Determine if specific subsets of a vector space are subspaces.\ne. Discuss the existence of a basis of an abstract vector space.\nf. Describe coordinates of a vector relative to a given basis.\ng. Determine a basis for and the dimension of a finite-dimensional space.\nh. Discuss spanning sets for vectors in Rn.\ni. Discuss linear independence for vectors in Rn.\nj. Define the dimension of a vector space.\n5. Orthogonality, projection, least-squares, orthogonal bases\na. Explain the Gram-Schmidt orthogonalization process.\nb. Define orthogonal projections.\nc. Define orthogonal complements.\nd. Compute the orthogonal projection of a vector onto a subspace, given a basis for the subspace.\ne. Explain how orthogonal projections relate to least square approximations.\n6. Linear combinations of polynomials, Bezier curves\na. Identify polynomials as generalized vectors.\nb. Explain linear combinations of basic polynomials.\nc. Describe orthogonality for polynomials.\nd. Distinguish between basic polynomials and Bernstein polynomials.\ne. Apply Bernstein polynomials to Bezier curves.\n7. Eigenvectors and eigenvalues\na. Find the eigenvalues and eigenvectors of a matrix.\nb. Define eigenvalues and eigenvectors geometrically.\nc. Use characteristic polynomials to compute eigenvalues and eigenvectors.\nd. Use eigenspaces of matrices, when possible, to diagonalize a matrix.\ne. Perform diagonalization of matrices.\nf. Explain the significance of eigenvectors and eigenvalues.\ng. Find the characteristic polynomial of a matrix.\nh. Use eigenvectors to represent a linear transformation with respect to a particularly nice basis.\n8. Applications to computer science: PCA, SVD, page-rank, graphics\na. Explain the geometric properties of PCA.\nb. Relate PCA to dimensionality reduction.\nc. Relate PCA to solving least-squares problems.\nd. Relate PCA to solving eigenvector problems.\ne. Apply PCA to reducing the dimensionality of a high-dimensional dataset (e.g., images).\nf. Explain the page-rank algorithm and understand how it relates to eigenvector problems.\ng. Explain the geometric differences between SVD and PCA.\nh. Apply SVD to a concrete example (e.g., movie rankings)."
  }
}